{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNMSAEoMo0VDx1qDrGkrINQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_-j14Pkm7BD",
        "outputId": "49e8212f-95d9-4c0a-b73c-c69b9aa02091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUqyQ_fyos9t",
        "outputId": "7b8a5648-63bc-4cc4-b03d-361f0f34a64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmprcxvf4ky\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For reference"
      ],
      "metadata": {
        "id": "BTlWxFhT34nU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -c \"--gpu-architecture sm_75 -O2 --default-stream per-thread\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void testKernel(float*x, int len)\n",
        "{\n",
        "\tint tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "\tif(tid < len) {\n",
        "\t\tfloat sum = x[tid];\n",
        "\t\tint iter = 0;\n",
        "\n",
        "\t\twhile(iter++ < len) {\n",
        "\t\t\tsum += 1;\n",
        "\t\t}\n",
        "\t\tx[tid] = sum;\n",
        "\t}\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void subKernel (float*a, float*b, float*c, int len)\n",
        "{\n",
        "\tint tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "\tif(tid < len) {\n",
        "\t\tc[tid] = a[tid] - b[tid];\n",
        "\t}\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\tconst int streamsNum = 2;\n",
        "\n",
        "\tint N=1<<10; // 1Kibi\n",
        "\n",
        "\tstd::cout << \"Running \" << N << \" (floats) as the input data size.\" << std::endl;\n",
        "\tstd::cout << \"Launching \" << streamsNum << \" cuda streams.\" << std::endl;\n",
        "\n",
        "\t// host\n",
        "\tfloat *h_a, *h_b, *h_c;\n",
        "\tcudaMallocHost((void**)&h_a, sizeof(float) * N);\n",
        "\tcudaMallocHost((void**)&h_b, sizeof(float) * N);\n",
        "\tcudaMallocHost((void**)&h_c, sizeof(float) * N);\n",
        "\n",
        "\tcudaMemset(h_a, 0, sizeof(float) * N);\n",
        "\tcudaMemset(h_b, 0, sizeof(float) * N);\n",
        "\n",
        "\t// device\n",
        "\tfloat *d_a, *d_b, *d_c;\n",
        "\n",
        "\tcudaMalloc((void**)&d_a, sizeof(float) * N);\n",
        "\tcudaMalloc((void**)&d_b, sizeof(float) * N);\n",
        "\tcudaMalloc((void**)&d_c, sizeof(float) * N);\n",
        "\n",
        "\t// streams\n",
        "\tcudaStream_t streams[streamsNum];\n",
        "\tcudaEvent_t  events[streamsNum]; // events for streams\n",
        "\n",
        "\tfor(int i=0; i<streamsNum; i++) {\n",
        "\t\tcudaStreamCreate(&streams[i]);\n",
        "\t\tcudaEventCreate(&events[i]);\n",
        "\t}\n",
        "\n",
        "\t// h2d\n",
        "\tcudaMemcpyAsync(d_a, h_a, sizeof(float)*N, cudaMemcpyHostToDevice, streams[0]);\n",
        "\tcudaMemcpyAsync(d_b, h_b, sizeof(float)*N, cudaMemcpyHostToDevice, streams[1]);\n",
        "\n",
        "\t// kernel\n",
        "\tdim3 block = dim3(128,1,1);\n",
        "\tdim3 grid = dim3((N + block.x - 1) / block.x,1,1);\n",
        "\n",
        "\ttestKernel <<< grid, block, 0, streams[0] >>> (d_a, N); // a + x\n",
        "\tcudaEventRecord(events[0], streams[0]);\n",
        "\ttestKernel <<< grid, block, 0, streams[1] >>> (d_b, N); // b + x\n",
        "\tcudaEventRecord(events[1], streams[1]);\n",
        "\n",
        "\tcudaEventSynchronize(events[0]);\n",
        "\tcudaEventSynchronize(events[1]);\n",
        "\n",
        "\tsubKernel <<< grid, block, 0, streams[0] >>> (d_a, d_b, d_c, N); // a - b\n",
        "\n",
        "\t// d2h\n",
        "\tcudaMemcpyAsync(h_c, d_c, sizeof(float)*N, cudaMemcpyDeviceToHost, streams[0]);\n",
        "\n",
        "\tcudaDeviceSynchronize(); // NOTE: this is needed to make sure prev dev opt is done!\n",
        "\n",
        "\tint error_c = 0;\n",
        "\tfor(int i=0; i<N; i++) {\n",
        "\t\tif(h_c[i] > 1e-8) {  // h_c should be 0\n",
        "\t\t\tprintf(\"h_c[%d] = %f\\n\",i, h_c[i]);\n",
        "\t\t\terror_c += 1;\n",
        "\t\t}\n",
        "\t}\n",
        "\tif(error_c == 0) {\n",
        "\t\tprintf(\"Pass test on h_c!\\n\");\n",
        "\t}\n",
        "\n",
        "\n",
        "\t// free\n",
        "\tfor(int i=0; i<streamsNum; i++) {\n",
        "\t\tcudaStreamDestroy(streams[i]);\n",
        "\t\tcudaEventDestroy(events[i]);\n",
        "\t}\n",
        "\n",
        "\tcudaFree(d_a);\n",
        "\tcudaFree(d_b);\n",
        "\tcudaFree(d_c);\n",
        "\n",
        "\tcudaFreeHost(h_a);\n",
        "\tcudaFreeHost(h_b);\n",
        "\tcudaFreeHost(h_c);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "metadata": {
        "id": "kuTukiOb33Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -c \"--gpu-architecture sm_75 -O2 --default-stream per-thread\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void testKernel(float*x, int len)\n",
        "{\n",
        "\tint tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "\tif(tid < len) {\n",
        "\t\tfloat sum = x[tid];\n",
        "\t\tint iter = 0;\n",
        "\n",
        "\t\twhile(iter++ < len) {\n",
        "\t\t\tsum += 1;\n",
        "\t\t}\n",
        "\t\tx[tid] = sum;\n",
        "\t}\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void subKernel (float*a, float*b, float*c, int len)\n",
        "{\n",
        "\tint tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "\tif(tid < len) {\n",
        "\t\tc[tid] = a[tid] - b[tid];\n",
        "\t}\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\tconst int streamsNum = 2;\n",
        "\n",
        "\tint N=1<<10; // 1Kibi\n",
        "\n",
        "\tstd::cout << \"Running \" << N << \" (floats) as the input data size.\" << std::endl;\n",
        "\tstd::cout << \"Launching \" << streamsNum << \" cuda streams.\" << std::endl;\n",
        "\n",
        "\t// host\n",
        "\tfloat *h_a, *h_b, *h_c;\n",
        "\tcudaMallocHost((void**)&h_a, sizeof(float) * N);\n",
        "\tcudaMallocHost((void**)&h_b, sizeof(float) * N);\n",
        "\tcudaMallocHost((void**)&h_c, sizeof(float) * N);\n",
        "\n",
        "\tcudaMemset(h_a, 0, sizeof(float) * N);\n",
        "\tcudaMemset(h_b, 0, sizeof(float) * N);\n",
        "\n",
        "\t// device\n",
        "\tfloat *d_a, *d_b, *d_c;\n",
        "\n",
        "\tcudaMalloc((void**)&d_a, sizeof(float) * N);\n",
        "\tcudaMalloc((void**)&d_b, sizeof(float) * N);\n",
        "\tcudaMalloc((void**)&d_c, sizeof(float) * N);\n",
        "\n",
        "\t// streams\n",
        "\tcudaStream_t streams[streamsNum];\n",
        "\tcudaEvent_t  events[streamsNum]; // events for streams\n",
        "\n",
        "\tfor(int i=0; i<streamsNum; i++) {\n",
        "\t\tcudaStreamCreate(&streams[i]);\n",
        "\t\tcudaEventCreate(&events[i]);\n",
        "\t}\n",
        "\n",
        "\t// h2d\n",
        "\tcudaMemcpyAsync(d_a, h_a, sizeof(float)*N, cudaMemcpyHostToDevice, streams[0]);\n",
        "\tcudaMemcpyAsync(d_b, h_b, sizeof(float)*N, cudaMemcpyHostToDevice, streams[1]);\n",
        "\n",
        "\t// kernel\n",
        "\tdim3 block = dim3(128,1,1);\n",
        "\tdim3 grid = dim3((N + block.x - 1) / block.x,1,1);\n",
        "\n",
        "\ttestKernel <<< grid, block, 0, streams[0] >>> (d_a, N); // a + x\n",
        "\tcudaEventRecord(events[0], streams[0]);\n",
        "\ttestKernel <<< grid, block, 0, streams[1] >>> (d_b, N); // b + x\n",
        "\tcudaEventRecord(events[1], streams[1]);\n",
        "\n",
        "\tcudaEventSynchronize(events[0]);\n",
        "\tcudaEventSynchronize(events[1]);\n",
        "\n",
        "\tsubKernel <<< grid, block, 0, streams[0] >>> (d_a, d_b, d_c, N); // a - b\n",
        "\n",
        "\t// d2h\n",
        "\tcudaMemcpyAsync(h_c, d_c, sizeof(float)*N, cudaMemcpyDeviceToHost, streams[0]);\n",
        "\n",
        "\tcudaDeviceSynchronize(); // NOTE: this is needed to make sure prev dev opt is done!\n",
        "\n",
        "\tint error_c = 0;\n",
        "\tfor(int i=0; i<N; i++) {\n",
        "\t\tif(h_c[i] > 1e-8) {  // h_c should be 0\n",
        "\t\t\tprintf(\"h_c[%d] = %f\\n\",i, h_c[i]);\n",
        "\t\t\terror_c += 1;\n",
        "\t\t}\n",
        "\t}\n",
        "\tif(error_c == 0) {\n",
        "\t\tprintf(\"Pass test on h_c!\\n\");\n",
        "\t}\n",
        "\n",
        "\n",
        "\t// free\n",
        "\tfor(int i=0; i<streamsNum; i++) {\n",
        "\t\tcudaStreamDestroy(streams[i]);\n",
        "\t\tcudaEventDestroy(events[i]);\n",
        "\t}\n",
        "\n",
        "\tcudaFree(d_a);\n",
        "\tcudaFree(d_b);\n",
        "\tcudaFree(d_c);\n",
        "\n",
        "\tcudaFreeHost(h_a);\n",
        "\tcudaFreeHost(h_b);\n",
        "\tcudaFreeHost(h_c);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJB99vXUmHMG",
        "outputId": "3a5f8b75-5016-41fd-973d-f8d69193f303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 1024 (floats) as the input data size.\n",
            "Launching 2 cuda streams.\n",
            "Pass test on h_c!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -c \"--gpu-architecture sm_75 -O2 --default-stream per-thread\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "__global__ void testKernel(float*x, int len)\n",
        "{\n",
        "\tint tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "\tif(tid < len) {\n",
        "\t\tfloat sum = x[tid];\n",
        "\t\tint iter = 0;\n",
        "\n",
        "\t\twhile(iter++ < len) {\n",
        "\t\t\tsum += 1;\n",
        "\t\t}\n",
        "\t\tx[tid] = sum;\n",
        "\t}\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void subKernel (float*a, float*b, float*c, int len)\n",
        "{\n",
        "\tint tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "\tif(tid < len) {\n",
        "\t\tc[tid] = a[tid] - b[tid];\n",
        "\t}\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "\tconst int streamsNum = 2;\n",
        "\n",
        "\tint N=1<<12; // 1Kibi\n",
        "\n",
        "\tstd::cout << \"Running \" << N << \" (floats) as the input data size.\" << std::endl;\n",
        "\tstd::cout << \"Launching \" << streamsNum << \" cuda streams.\" << std::endl;\n",
        "\n",
        "\t// host\n",
        "\tfloat *h_a, *h_b, *h_c;\n",
        "\tcudaMallocHost((void**)&h_a, sizeof(float) * N);\n",
        "\tcudaMallocHost((void**)&h_b, sizeof(float) * N);\n",
        "\tcudaMallocHost((void**)&h_c, sizeof(float) * N);\n",
        "\n",
        "\tcudaMemset(h_a, 0, sizeof(float) * N);\n",
        "\tcudaMemset(h_b, 0, sizeof(float) * N);\n",
        "\n",
        "\t// device\n",
        "\tfloat *d_a, *d_b, *d_c;\n",
        "\n",
        "\tcudaMalloc((void**)&d_a, sizeof(float) * N);\n",
        "\tcudaMalloc((void**)&d_b, sizeof(float) * N);\n",
        "\tcudaMalloc((void**)&d_c, sizeof(float) * N);\n",
        "\n",
        "\t// streams\n",
        "\tcudaStream_t streams[streamsNum];\n",
        "\t//cudaEvent_t  events[streamsNum]; // events for streams\n",
        "\n",
        "\tfor(int i=0; i<streamsNum; i++) {\n",
        "\t\tcudaStreamCreate(&streams[i]);\n",
        "\t\t//cudaEventCreate(&events[i]);\n",
        "\t}\n",
        "\n",
        "\t// h2d\n",
        "\tcudaMemcpyAsync(d_a, h_a, sizeof(float)*N, cudaMemcpyHostToDevice, streams[0]);\n",
        "\tcudaMemcpyAsync(d_b, h_b, sizeof(float)*N, cudaMemcpyHostToDevice, streams[1]);\n",
        "\n",
        "\t// kernel\n",
        "\tdim3 block = dim3(128,1,1);\n",
        "\tdim3 grid = dim3((N + block.x - 1) / block.x,1,1);\n",
        "\n",
        "\ttestKernel <<< grid, block, 0, streams[0] >>> (d_a, N); // a + x\n",
        "\t//cudaEventRecord(events[0], streams[0]);\n",
        "\ttestKernel <<< grid, block, 0, streams[1] >>> (d_b, N); // b + x\n",
        "\t//cudaEventRecord(events[1], streams[1]);\n",
        "\n",
        "\t//cudaEventSynchronize(events[0]);\n",
        "\t//cudaEventSynchronize(events[1]);\n",
        "\n",
        "\tsubKernel <<< grid, block, 0, streams[0] >>> (d_a, d_b, d_c, N); // a - b\n",
        "\n",
        "\t// d2h\n",
        "\tcudaMemcpyAsync(h_c, d_c, sizeof(float)*N, cudaMemcpyDeviceToHost, streams[0]);\n",
        "\n",
        "\t//cudaDeviceSynchronize(); // NOTE: this is needed to make sure prev dev opt is done!\n",
        "\n",
        "\tint error_c = 0;\n",
        "\tfor(int i=0; i<N; i++) {\n",
        "\t\tif(h_c[i] > 1e-8) {  // h_c should be 0\n",
        "\t\t\tprintf(\"h_c[%d] = %f\\n\",i, h_c[i]);\n",
        "\t\t\terror_c += 1;\n",
        "\t\t}\n",
        "\t}\n",
        "\tif(error_c == 0) {\n",
        "\t\tprintf(\"Pass test on h_c!\\n\");\n",
        "\t}\n",
        "\n",
        "\n",
        "\t// free\n",
        "\tfor(int i=0; i<streamsNum; i++) {\n",
        "\t\tcudaStreamDestroy(streams[i]);\n",
        "\t\t//cudaEventDestroy(events[i]);\n",
        "\t}\n",
        "\n",
        "\tcudaFree(d_a);\n",
        "\tcudaFree(d_b);\n",
        "\tcudaFree(d_c);\n",
        "\n",
        "\tcudaFreeHost(h_a);\n",
        "\tcudaFreeHost(h_b);\n",
        "\tcudaFreeHost(h_c);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "metadata": {
        "id": "JbBxcLfDw-yk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d32b29e-f0da-41b0-bc24-ef250d11110a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 4096 (floats) as the input data size.\n",
            "Launching 2 cuda streams.\n",
            "Pass test on h_c!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -c \"--gpu-architecture sm_75 -O2 --default-stream per-thread\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Kernel with a heavy workload (more iterations)\n",
        "__global__ void testKernel(float* x, int len)\n",
        "{\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (tid < len) {\n",
        "        float sum = x[tid];\n",
        "        // Increase iterations to force longer execution time\n",
        "        int iter = 0;\n",
        "        while(iter++ < len * 100) {  // increased workload factor (100x)\n",
        "            sum += 1;\n",
        "        }\n",
        "        x[tid] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void subKernel (float* a, float* b, float* c, int len)\n",
        "{\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if(tid < len) {\n",
        "        c[tid] = a[tid] - b[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    const int streamsNum = 2;\n",
        "\n",
        "    // Use a larger N to further delay kernel execution\n",
        "    int N = 1 << 20; // 1M elements\n",
        "\n",
        "    std::cout << \"Running \" << N << \" (floats) as the input data size.\" << std::endl;\n",
        "    std::cout << \"Launching \" << streamsNum << \" cuda streams.\" << std::endl;\n",
        "\n",
        "    // Allocate pinned host memory\n",
        "    float *h_a, *h_b, *h_c;\n",
        "    cudaMallocHost((void**)&h_a, sizeof(float) * N);\n",
        "    cudaMallocHost((void**)&h_b, sizeof(float) * N);\n",
        "    cudaMallocHost((void**)&h_c, sizeof(float) * N);\n",
        "\n",
        "    cudaMemset(h_a, 0, sizeof(float) * N);\n",
        "    cudaMemset(h_b, 0, sizeof(float) * N);\n",
        "\n",
        "    // Allocate device memory\n",
        "    float *d_a, *d_b, *d_c;\n",
        "    cudaMalloc((void**)&d_a, sizeof(float) * N);\n",
        "    cudaMalloc((void**)&d_b, sizeof(float) * N);\n",
        "    cudaMalloc((void**)&d_c, sizeof(float) * N);\n",
        "\n",
        "    // Create two streams\n",
        "    cudaStream_t streams[streamsNum];\n",
        "    for(int i=0; i<streamsNum; i++) {\n",
        "        cudaStreamCreate(&streams[i]);\n",
        "    }\n",
        "\n",
        "    // Asynchronously copy from host to device\n",
        "    cudaMemcpyAsync(d_a, h_a, sizeof(float)*N, cudaMemcpyHostToDevice, streams[0]);\n",
        "    cudaMemcpyAsync(d_b, h_b, sizeof(float)*N, cudaMemcpyHostToDevice, streams[1]);\n",
        "\n",
        "    // Set up execution configuration\n",
        "    dim3 block(128, 1, 1);\n",
        "    dim3 grid((N + block.x - 1) / block.x, 1, 1);\n",
        "\n",
        "    // Launch kernels in separate streams\n",
        "    testKernel<<< grid, block, 0, streams[0] >>>(d_a, N);\n",
        "    testKernel<<< grid, block, 0, streams[1] >>>(d_b, N);\n",
        "\n",
        "    // Launch subtraction kernel in stream[0]\n",
        "    subKernel<<< grid, block, 0, streams[0] >>>(d_a, d_b, d_c, N);\n",
        "\n",
        "    // Asynchronously copy result back to host in stream[0]\n",
        "    cudaMemcpyAsync(h_c, d_c, sizeof(float)*N, cudaMemcpyDeviceToHost, streams[0]);\n",
        "\n",
        "    // No synchronization! Immediately access h_c on the host.\n",
        "    // This loop may access h_c before the device work is finished.\n",
        "    int error_c = 0;\n",
        "    for(int i = 0; i < N; i++) {\n",
        "        if (h_c[i] > 1e-8) {  // Expected: h_c should be 0\n",
        "            printf(\"h_c[%d] = %f\\n\", i, h_c[i]);\n",
        "            error_c++;\n",
        "        }\n",
        "    }\n",
        "    if(error_c == 0) {\n",
        "        printf(\"Pass test on h_c! (but this is by chance without sync)\\n\");\n",
        "    } else {\n",
        "        printf(\"Test failed: detected %d errors in h_c (race condition)!\\n\", error_c);\n",
        "    }\n",
        "\n",
        "    // Clean up: destroy streams and free memory\n",
        "    for(int i=0; i<streamsNum; i++) {\n",
        "        cudaStreamDestroy(streams[i]);\n",
        "    }\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    cudaFreeHost(h_a);\n",
        "    cudaFreeHost(h_b);\n",
        "    cudaFreeHost(h_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2MF3T_ms196",
        "outputId": "02550d40-3f29-4a8f-ad92-2edec6b97168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 1048576 (floats) as the input data size.\n",
            "Launching 2 cuda streams.\n",
            "Pass test on h_c! (but this is by chance without sync)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dSCLKDXa1bdt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}